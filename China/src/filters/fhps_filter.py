"""
FHPS (åˆ†çº¢æ´¾æ¯é€è‚¡) filtering and analysis for Chinese equity markets.

This module provides a FhpsFilter class that encapsulates asynchronous functions
to filter and analyze Chinese stocks with dividend/split plans. It processes
stocks with ex-dividend dates, calculates performance metrics, and adds fund flow
data for comprehensive analysis.
"""

import asyncio
import os
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple

import pandas as pd
import yaml
from pydantic import BaseModel

from src.api.akshare import (
    StockIndividualFundFlowAPI,
    StockIndividualFundFlowConfig,
    get_market_by_stock_code,
)

# Import settings first to disable tqdm before akshare import
from src.settings import configure_environment
from src.utilities.logger import get_logger
from src.utilities.trading_calendar import get_previous_trading_day

configure_environment()

# akshare imports now handled through centralized API modules
import akshare as ak

if TYPE_CHECKING:
    from rich.progress import Progress, TaskID

# Initialize logger for this module
logger = get_logger("fhps_filter")

# Create a semaphore to limit concurrent requests
REQUEST_SEMAPHORE = asyncio.Semaphore(10)


class FileConfig(BaseModel):
    """
    Configuration model for file-related settings.

    This model handles file configuration metadata including config name,
    version, and description.
    """

    config_name: str = "PROD"
    description: str = ""
    version: str = ""


class FhpsFilterConfig(BaseModel):
    """
    Configuration model for FhpsFilter class parameters.

    This model validates and provides default values for the FhpsFilter class constants.
    """

    # FHPS data configuration
    fhps_date: str = ""  # Date for FHPS data query (YYYYMMDD format)
    min_transfer_ratio: float = 1.0  # Minimum transfer ratio to filter
    max_price_change_percent: float = (
        10.0  # Maximum price change percentage for filtering
    )

    # Processing configuration
    batch_size: int = 10  # Batch size for concurrent processing

    # Output configuration
    report_dir: str = "output/reports/filters/fhps_filter"
    output_filename_template: str = "é™¤æƒé™¤æ¯è‚¡ç¥¨-{date}.csv"


class Config(BaseModel):
    """
    Configuration model for nested YAML structure supporting both akshare and FhpsFilter configs.

    This model handles the nested structure from input/config/filters/fhps_filter.
    """

    akshare: Dict[str, Dict[str, Any]] = {}
    fhps_filter: FhpsFilterConfig = FhpsFilterConfig()
    file_config: FileConfig = FileConfig()


def load_config(
    config_name: Optional[str] = None,
) -> Tuple[StockIndividualFundFlowConfig, FhpsFilterConfig, FileConfig]:
    """
    Load nested configuration from YAML file.

    Args:
        config_name: YAML config file name. If None, uses default config

    Returns:
        tuple: (akshare_config, fhps_filter_config, file_config)

    Raises:
        FileNotFoundError: If config file doesn't exist
        ValueError: If config validation fails
    """
    config_dir = Path("input/config/filters/fhps_filter/")
    config_name = config_name or "config"
    config_path = config_dir / f"{config_name}.yml"

    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    # Load YAML config
    with open(config_path, "r", encoding="utf-8") as f:
        config_data = yaml.safe_load(f)

    # Check if it's nested format (has 'akshare' key) or flat format
    if "akshare" in config_data:
        # Nested format - extract each section
        configs = Config(**config_data)
        akshare_config = StockIndividualFundFlowConfig(
            **configs.akshare.get("stock_individual_fund_flow", {})
        )
        fhps_filter_config = configs.fhps_filter
        file_config = configs.file_config
    else:
        # Flat format - assume all config is for akshare
        akshare_config = StockIndividualFundFlowConfig(**config_data)
        fhps_filter_config = FhpsFilterConfig()
        file_config = FileConfig()

    return akshare_config, fhps_filter_config, file_config


class FhpsFilter:
    """
    Filter and analyze stocks with dividend/split plans (FHPS).

    This class processes stocks with ex-dividend dates, calculates performance metrics,
    and enriches data with fund flow information for comprehensive analysis.
    """

    def __init__(
        self,
        industry_stock_mapping_df: pd.DataFrame,
        stock_zh_a_spot_em_df: pd.DataFrame,
        config_name: Optional[str] = None,
    ):
        """
        Initialize FhpsFilter with market data and configuration.

        Args:
            industry_stock_mapping_df: DataFrame containing industry-stock mapping
            stock_zh_a_spot_em_df: DataFrame containing stock market data
            config_name: Optional config name to use for loading configuration
        """
        self.industry_stock_mapping_df = industry_stock_mapping_df
        self.stock_zh_a_spot_em_df = stock_zh_a_spot_em_df

        # Load configuration
        self.akshare_config, self.filter_config, self.file_config = load_config(
            config_name
        )

        # Apply class constants from config
        # Default to end of year if no date specified (more likely to have FHPS data)
        self.FHPS_DATE = self.filter_config.fhps_date or "20241231"
        self.MIN_TRANSFER_RATIO = self.filter_config.min_transfer_ratio
        self.MAX_PRICE_CHANGE_PERCENT = self.filter_config.max_price_change_percent
        self.BATCH_SIZE = self.filter_config.batch_size
        self.REPORT_DIR = self.filter_config.report_dir
        self.OUTPUT_FILENAME_TEMPLATE = self.filter_config.output_filename_template

        self.logger = get_logger("fhps_filter")

    async def _get_cached_fhps_data(self) -> Optional[pd.DataFrame]:
        """
        Get FHPS data with caching support.

        Returns:
            DataFrame with FHPS data or None if no data available
        """
        # Define cache paths
        cache_dir = "data/fhps"
        os.makedirs(cache_dir, exist_ok=True)
        cache_filename = f"stock_fhps_em-{self.FHPS_DATE}.csv"
        cache_path = os.path.join(cache_dir, cache_filename)

        # Check if cached file exists
        if os.path.exists(cache_path):
            self.logger.info(
                f"âœ… CACHED DATA FOUND! Loading FHPS data from: {cache_path}"
            )
            print(f"âœ… CACHED DATA FOUND! Loading FHPS data from: {cache_path}")
            try:
                df = pd.read_csv(cache_path, encoding="utf-8-sig")
                # Ensure stock codes are strings with proper 6-digit format
                if "ä»£ç " in df.columns:
                    df["ä»£ç "] = df["ä»£ç "].apply(lambda x: str(x).zfill(6))
                self.logger.info(
                    f"âœ… Successfully loaded {len(df)} FHPS records from cache"
                )
                print(f"âœ… Successfully loaded {len(df)} FHPS records from cache")
                return df
            except Exception as e:
                self.logger.error(f"âŒ Error loading cached FHPS data: {e}")
                print(f"âŒ Error loading cached FHPS data: {e}")
                self.logger.info("ğŸ”„ Falling back to API fetch...")
                print("ğŸ”„ Falling back to API fetch...")
                # Fall through to fetch fresh data

        # Fetch fresh data from API
        self.logger.info(
            f"ğŸŒ NO CACHE FOUND - Fetching fresh FHPS data from API for date: {self.FHPS_DATE}"
        )
        print(
            f"ğŸŒ NO CACHE FOUND - Fetching fresh FHPS data from API for date: {self.FHPS_DATE}"
        )
        try:
            stock_fhps_em_df = await asyncio.to_thread(
                ak.stock_fhps_em, date=self.FHPS_DATE
            )

            if stock_fhps_em_df is not None and not stock_fhps_em_df.empty:
                # Ensure stock codes are strings with proper 6-digit format
                if "ä»£ç " in stock_fhps_em_df.columns:
                    stock_fhps_em_df["ä»£ç "] = stock_fhps_em_df["ä»£ç "].apply(
                        lambda x: str(x).zfill(6)
                    )

                # Cache the data
                self.logger.info(
                    f"ğŸ’¾ Caching {len(stock_fhps_em_df)} FHPS records to: {cache_path}"
                )
                print(
                    f"ğŸ’¾ Caching {len(stock_fhps_em_df)} FHPS records to: {cache_path}"
                )
                stock_fhps_em_df.to_csv(cache_path, index=False, encoding="utf-8-sig")
                self.logger.info("âœ… FHPS data successfully cached for future use")
                print("âœ… FHPS data successfully cached for future use")

            return stock_fhps_em_df

        except Exception as api_error:
            error_msg = (
                f"Failed to fetch FHPS data for date {self.FHPS_DATE}: {str(api_error)}"
            )
            self.logger.error(error_msg)
            return None

    async def _get_cached_filtered_fhps_data(self) -> Optional[pd.DataFrame]:
        """
        Caching: Get filtered FHPS data with pre-cached historical prices.

        Returns:
            DataFrame with filtered FHPS data including é™¤æƒé™¤æ¯æ—¥è‚¡ä»· column, or None if not available
        """
        # Define filtered cache paths
        cache_dir = "data/fhps"
        os.makedirs(cache_dir, exist_ok=True)
        filtered_cache_filename = f"stock_fhps_em_filtered-{self.FHPS_DATE}.csv"
        filtered_cache_path = os.path.join(cache_dir, filtered_cache_filename)

        # Check if filtered cache exists
        if os.path.exists(filtered_cache_path):
            self.logger.info(
                f"ğŸš€ FILTERED CACHE FOUND! Loading filtered FHPS data from: {filtered_cache_path}"
            )
            print(
                f"ğŸš€ FILTERED CACHE FOUND! Loading filtered FHPS data from: {filtered_cache_path}"
            )
            try:
                # Load as CSV
                df = pd.read_csv(filtered_cache_path, encoding="utf-8-sig")
                # Ensure stock codes are strings with proper 6-digit format
                if "ä»£ç " in df.columns:
                    df["ä»£ç "] = df["ä»£ç "].apply(lambda x: str(x).zfill(6))
                # Convert date column back to datetime
                if "é™¤æƒé™¤æ¯æ—¥" in df.columns:
                    df["é™¤æƒé™¤æ¯æ—¥"] = pd.to_datetime(df["é™¤æƒé™¤æ¯æ—¥"])
                self.logger.info(
                    f"ğŸš€ Successfully loaded {len(df)} filtered FHPS records with cached prices"
                )
                print(
                    f"ğŸš€ Successfully loaded {len(df)} filtered FHPS records with cached prices"
                )
                return df
            except Exception as e:
                self.logger.error(f"âŒ Error loading filtered FHPS cache: {e}")
                print(f"âŒ Error loading filtered FHPS cache: {e}")
                # Fall through to create filtered cache

        # No filtered cache found, need to create it
        self.logger.info("ğŸ“¦ Creating filtered cache with historical prices...")
        print("ğŸ“¦ Creating filtered cache with historical prices...")

        # Get raw FHPS data first
        stock_fhps_em_df = await self._get_cached_fhps_data()
        if stock_fhps_em_df is None or stock_fhps_em_df.empty:
            return None

        # Filter stocks with transfer ratios (remove NaN values)
        df = stock_fhps_em_df.dropna(subset=["é€è½¬è‚¡ä»½-é€è½¬æ€»æ¯”ä¾‹"])
        self.logger.info(
            f"ğŸ“Š After filtering: {len(df)} stocks with valid transfer ratios"
        )
        print(f"ğŸ“Š After filtering: {len(df)} stocks with valid transfer ratios")

        # Convert ex-dividend date to datetime if not already
        if "é™¤æƒé™¤æ¯æ—¥" in df.columns:
            df.loc[:, "é™¤æƒé™¤æ¯æ—¥"] = pd.to_datetime(
                df["é™¤æƒé™¤æ¯æ—¥"], format="%Y-%m-%d"
            )

        # Filter stocks with ex-dividend dates before today
        today = datetime.today()
        filter_past = df.loc[:, "é™¤æƒé™¤æ¯æ—¥"] < today
        df_filtered = df[filter_past]
        self.logger.info(
            f"ğŸ“Š After date filtering: {len(df_filtered)} stocks with past ex-dividend dates"
        )
        print(
            f"ğŸ“Š After date filtering: {len(df_filtered)} stocks with past ex-dividend dates"
        )

        # Pre-fetch historical prices for all filtered stocks
        self.logger.info("ğŸ’° Pre-fetching historical prices for all filtered stocks...")
        print("ğŸ’° Pre-fetching historical prices for all filtered stocks...")

        # Add é™¤æƒé™¤æ¯æ—¥è‚¡ä»· column
        ex_prices = []
        for _, row in df_filtered.iterrows():
            stock_code = str(row["ä»£ç "]).zfill(6)
            ex_date = row["é™¤æƒé™¤æ¯æ—¥"]
            try:
                ex_price = await self.get_stock_price_async(stock_code, ex_date)
                ex_prices.append(ex_price)
            except Exception as e:
                self.logger.warning(
                    f"Failed to get price for {stock_code} on {ex_date}: {e}"
                )
                ex_prices.append(None)

        # Add the historical prices column
        df_filtered = df_filtered.copy()
        df_filtered.loc[:, "é™¤æƒé™¤æ¯æ—¥è‚¡ä»·"] = ex_prices

        # Cache the filtered data with historical prices
        try:
            self.logger.info(
                f"ğŸ’¾ Caching {len(df_filtered)} filtered FHPS records with historical prices to: {filtered_cache_path}"
            )
            print(
                f"ğŸ’¾ Caching {len(df_filtered)} filtered FHPS records with historical prices to: {filtered_cache_path}"
            )
            df_filtered.to_csv(filtered_cache_path, index=False, encoding="utf-8-sig")
            self.logger.info("âœ… Filtered cache created successfully!")
            print("âœ… Filtered cache created successfully!")
        except Exception as e:
            self.logger.error(f"âŒ Error creating filtered cache: {e}")
            print(f"âŒ Error creating filtered cache: {e}")

        return df_filtered

    async def get_stock_price_async(
        self, stock_code: str, date: datetime
    ) -> Optional[float]:
        """
        Get stock price asynchronously for the previous trading day before the given date.
        Uses trading calendar to find valid trading days and implements robust fallback.

        Args:
            stock_code: Stock code (6-digit format)
            date: Ex-dividend date (will get price from previous trading day)

        Returns:
            Stock price from previous trading day or None if not found
        """
        max_attempts = 5  # Prevent infinite loops
        
        # Get the previous trading day before the ex-dividend date
        current_date = get_previous_trading_day(date)
        
        for _ in range(max_attempts):
            try:
                async with REQUEST_SEMAPHORE:
                    # Use asyncio.to_thread for non-blocking akshare calls
                    df_price = await asyncio.to_thread(
                        ak.stock_zh_a_hist,
                        stock_code,
                        period="daily",
                        start_date=current_date.strftime("%Y%m%d"),
                        end_date=current_date.strftime("%Y%m%d"),
                        adjust="",  # ä¸å¤æƒ - for accurate fill-right analysis
                    )
                    if not df_price.empty:
                        price = df_price["æ”¶ç›˜"].iloc[0]  # Close price
                        self.logger.debug(f"Found price for {stock_code} on {current_date.strftime('%Y-%m-%d')}: {price}")
                        return price
                    
                    # No data for this date, try previous trading day
                    self.logger.debug(f"No price data for {stock_code} on {current_date.strftime('%Y-%m-%d')}, trying previous trading day")
                    current_date = get_previous_trading_day(current_date)
                    
            except Exception as e:
                self.logger.error(f"Error fetching price for {stock_code} on {current_date}: {e}")
                # Try previous trading day on API error too
                current_date = get_previous_trading_day(current_date)
        
        self.logger.warning(f"Could not find price for {stock_code} after {max_attempts} attempts")
        return None

    async def get_fund_flow_data(self, stock_code: str) -> Dict[str, Optional[float]]:
        """
        Get fund flow data for a stock using the configured periods.

        Args:
            stock_code: Stock code (6-digit format)

        Returns:
            Dictionary containing fund flow data for different periods
        """
        fund_flow_api = StockIndividualFundFlowAPI(self.akshare_config)

        try:
            # Determine market from stock code
            market = get_market_by_stock_code(stock_code)

            # Get fund flow DataFrame for the stock
            fund_flow_df = await fund_flow_api.fetch_async(stock_code, market)

            # Process the data for the configured periods
            fund_flows, price_changes = fund_flow_api.process_periods(
                fund_flow_df, self.akshare_config.period_count
            )

            result = {}
            for i, period in enumerate(self.akshare_config.period_count):
                # Fund flow data (already in äº¿)
                if i < len(fund_flows):
                    result[f"{period}æ—¥ä¸»åŠ›å‡€æµå…¥-æ€»å‡€é¢(äº¿)"] = fund_flows[i]
                else:
                    result[f"{period}æ—¥ä¸»åŠ›å‡€æµå…¥-æ€»å‡€é¢(äº¿)"] = None

                # Price change data
                if i < len(price_changes):
                    result[f"{period}æ—¥æ¶¨è·Œå¹…(%)"] = price_changes[i]
                else:
                    result[f"{period}æ—¥æ¶¨è·Œå¹…(%)"] = None

            return result

        except Exception as e:
            self.logger.error(f"Error fetching fund flow data for {stock_code}: {e}")
            # Return default structure with None values
            result = {}
            for period in self.akshare_config.period_count:
                result[f"{period}æ—¥ä¸»åŠ›å‡€æµå…¥-æ€»å‡€é¢(äº¿)"] = None
                result[f"{period}æ—¥æ¶¨è·Œå¹…(%)"] = None
            return result

    def get_stock_market_data(self, stock_code: str) -> Dict[str, Any]:
        """
        Get market data for a stock from the spot market DataFrame.

        Args:
            stock_code: Stock code (6-digit format)

        Returns:
            Dictionary containing market data (market cap, P/E, P/B, etc.)
        """
        try:
            stock_row = self.stock_zh_a_spot_em_df[
                self.stock_zh_a_spot_em_df["ä»£ç "] == stock_code
            ]

            if stock_row.empty:
                self.logger.warning(f"No market data found for stock {stock_code}")
                return {
                    "æ€»å¸‚å€¼(äº¿)": None,
                    "æµé€šå¸‚å€¼(äº¿)": None,
                    "å¸‚ç›ˆç‡-åŠ¨æ€": None,
                    "å¸‚å‡€ç‡": None,
                    "60æ—¥æ¶¨è·Œå¹…(%)": None,
                    "å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…(%)": None,
                }

            row = stock_row.iloc[0]

            # Convert market cap values to äº¿ and round to 2 decimal places
            total_market_cap = row.get("æ€»å¸‚å€¼", None)
            if total_market_cap is not None:
                total_market_cap = round(total_market_cap / 1e8, 2)

            circulating_market_cap = row.get("æµé€šå¸‚å€¼", None)
            if circulating_market_cap is not None:
                circulating_market_cap = round(circulating_market_cap / 1e8, 2)

            return {
                "æ€»å¸‚å€¼(äº¿)": total_market_cap,
                "æµé€šå¸‚å€¼(äº¿)": circulating_market_cap,
                "å¸‚ç›ˆç‡-åŠ¨æ€": row.get("å¸‚ç›ˆç‡-åŠ¨æ€", None),
                "å¸‚å‡€ç‡": row.get("å¸‚å‡€ç‡", None),
                "60æ—¥æ¶¨è·Œå¹…(%)": row.get("60æ—¥æ¶¨è·Œå¹…", None),
                "å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…(%)": row.get("å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…", None),
            }

        except Exception as e:
            self.logger.error(f"Error getting market data for {stock_code}: {e}")
            return {
                "æ€»å¸‚å€¼(äº¿)": None,
                "æµé€šå¸‚å€¼(äº¿)": None,
                "å¸‚ç›ˆç‡-åŠ¨æ€": None,
                "å¸‚å‡€ç‡": None,
                "60æ—¥æ¶¨è·Œå¹…(%)": None,
                "å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…(%)": None,
            }

    def get_stock_industry(self, stock_code: str) -> str:
        """
        Get industry for a stock from the industry mapping DataFrame.

        Args:
            stock_code: Stock code (6-digit format)

        Returns:
            Industry name or "æœªçŸ¥" if not found
        """
        try:
            industry_row = self.industry_stock_mapping_df[
                self.industry_stock_mapping_df["ä»£ç "] == stock_code
            ]

            if industry_row.empty:
                return "æœªçŸ¥"

            return industry_row.iloc[0].get("è¡Œä¸š", "æœªçŸ¥")

        except Exception as e:
            self.logger.error(f"Error getting industry for {stock_code}: {e}")
            return "æœªçŸ¥"

    def get_today_stock_price(self, stock_code: str) -> Optional[float]:
        """
        Get today's stock price from cached market data.

        Args:
            stock_code: Stock code (6-digit format)

        Returns:
            Current stock price or None if not found
        """
        try:
            stock_row = self.stock_zh_a_spot_em_df[
                self.stock_zh_a_spot_em_df["ä»£ç "] == stock_code
            ]

            if stock_row.empty:
                return None

            return stock_row.iloc[0].get("æœ€æ–°ä»·", None)

        except Exception as e:
            self.logger.error(f"Error getting today's price for {stock_code}: {e}")
            return None

    async def run_analysis(
        self,
        _progress: Optional["Progress"] = None,
        _parent_task_id: Optional["TaskID"] = None,
    ) -> None:
        """
        Run the complete FHPS filter analysis.

        Args:
            _progress: Optional Progress instance for tracking
            _parent_task_id: Optional parent task ID
        """
        self.logger.info("Starting FHPS filter analysis")
        self.logger.info(
            f"Progress params: _progress={_progress is not None}, _parent_task_id={_parent_task_id}"
        )
        self.logger.info("About to enter try block")

        try:
            # Update progress
            if _progress and _parent_task_id:
                try:
                    _progress.update(
                        _parent_task_id,
                        completed=10,
                        description="ğŸ“Š Fetching FHPS data...",
                    )
                    self.logger.info("Progress updated to 10%")
                except Exception as e:
                    self.logger.error(f"Failed to update progress to 10%: {e}")

            # Fetch filtered FHPS data with cached historical prices
            df_filtered = await self._get_cached_filtered_fhps_data()

            if df_filtered is None or df_filtered.empty:
                self.logger.warning(
                    f"No filtered FHPS data found for date {self.FHPS_DATE}"
                )
                if _progress and _parent_task_id:
                    _progress.update(
                        _parent_task_id,
                        completed=100,
                        description="âš ï¸ No FHPS data available for the specified date",
                    )
                return

            self.logger.info(
                f"âœ… Using filtered FHPS data with {len(df_filtered)} pre-processed records"
            )

            # Update progress immediately when cached data is found
            if _progress and _parent_task_id:
                try:
                    _progress.update(
                        _parent_task_id,
                        completed=50,
                        description="ğŸš€ Using cached FHPS data, applying filters...",
                    )
                    self.logger.info("Progress updated to 50% - using cached data")
                except Exception as e:
                    self.logger.error(f"Failed to update progress to 50%: {e}")

            self.logger.info(
                f"After ex-dividend date filter (< today): {len(df_filtered)} stocks"
            )

            # Apply minimum transfer ratio filter
            df_filtered = df_filtered[
                df_filtered["é€è½¬è‚¡ä»½-é€è½¬æ€»æ¯”ä¾‹"] >= self.MIN_TRANSFER_RATIO
            ]
            self.logger.info(
                f"After transfer ratio filter (>= {self.MIN_TRANSFER_RATIO}): {len(df_filtered)} stocks"
            )

            if _progress and _parent_task_id:
                _progress.update(
                    _parent_task_id,
                    completed=30,
                    description=f"ğŸ“ˆ Getting prices for {len(df_filtered)} filtered FHPS stocks...",
                )

            # First, get basic price data for all FHPS stocks to apply price change filter
            price_results = []

            # Process stocks in smaller batches for price data only
            stock_list = list(df_filtered.iterrows())
            price_batches = [
                stock_list[i : i + self.BATCH_SIZE]
                for i in range(0, len(stock_list), self.BATCH_SIZE)
            ]

            self.logger.info(
                f"Getting prices for {len(df_filtered)} FHPS stocks in {len(price_batches)} batches"
            )

            # Create a nested progress bar for detailed batch tracking if available
            batch_progress_task = None
            if _progress:
                batch_progress_task = _progress.add_task(
                    "    ğŸ“Š Processing FHPS price batches",
                    total=len(price_batches),
                    visible=True,
                )

            # Check if we're using cached prices for faster progress updates
            has_cached_prices = (
                "é™¤æƒé™¤æ¯æ—¥è‚¡ä»·" in df_filtered.columns
                and not df_filtered["é™¤æƒé™¤æ¯æ—¥è‚¡ä»·"].isna().all()
            )

            for batch_idx, batch in enumerate(price_batches):
                if _progress and _parent_task_id:
                    progress_pct = 30 + (batch_idx / len(price_batches)) * 40
                    cache_indicator = " ğŸš€" if has_cached_prices else ""
                    _progress.update(
                        _parent_task_id,
                        completed=progress_pct,
                        description=f"ğŸ“ˆ Processing price batch {batch_idx + 1}/{len(price_batches)}{cache_indicator}...",
                    )

                # Update batch progress
                if _progress and batch_progress_task:
                    _progress.update(
                        batch_progress_task,
                        completed=batch_idx,
                        description=f"Batch {batch_idx + 1}/{len(price_batches)}: {len(batch)} stocks",
                    )

                # Get basic price data for this batch
                for stock_idx, (original_idx, row) in enumerate(batch):
                    stock_code = str(row["ä»£ç "]).zfill(
                        6
                    )  # Convert to string and pad to 6 digits
                    ex_date = row["é™¤æƒé™¤æ¯æ—¥"]

                    # Update batch progress with current stock
                    if _progress and batch_progress_task:
                        _progress.update(
                            batch_progress_task,
                            description=f"Batch {batch_idx + 1}/{len(price_batches)}: {stock_code} ({stock_idx + 1}/{len(batch)})",
                        )

                    try:
                        # Get ex-dividend price from cache if available (filtered cache optimization)
                        ex_price = None
                        using_cached_price = False
                        if "é™¤æƒé™¤æ¯æ—¥è‚¡ä»·" in row and pd.notna(row["é™¤æƒé™¤æ¯æ—¥è‚¡ä»·"]):
                            ex_price = row["é™¤æƒé™¤æ¯æ—¥è‚¡ä»·"]
                            using_cached_price = True
                            # print(f"ğŸš€ Using cached ex-dividend price for {stock_code}: {ex_price}")
                        else:
                            # Fallback to API if not cached
                            ex_price = await self.get_stock_price_async(
                                stock_code, ex_date
                            )
                            # print(f"ğŸŒ Fetched ex-dividend price from API for {stock_code}: {ex_price}")

                        # Update progress more frequently when using cached prices
                        if (
                            using_cached_price
                            and _progress
                            and batch_progress_task
                            and stock_idx % 5 == 0
                        ):
                            _progress.update(
                                batch_progress_task,
                                description=f"Batch {batch_idx + 1}/{len(price_batches)}: {stock_code} ({stock_idx + 1}/{len(batch)}) [CACHED]",
                            )

                        # Get today's price from cached market data (much faster)
                        today_price = self.get_today_stock_price(stock_code)

                        # Calculate price change
                        price_change_pct = None
                        if (
                            ex_price is not None
                            and today_price is not None
                            and ex_price != 0
                        ):
                            price_change_pct = round(
                                ((today_price - ex_price) / ex_price) * 100, 2
                            )

                        if price_change_pct is not None:
                            price_results.append(
                                {
                                    "original_idx": original_idx,
                                    "row": row,
                                    "ex_price": ex_price,
                                    "today_price": today_price,
                                    "price_change_pct": price_change_pct,
                                }
                            )

                    except Exception as e:
                        self.logger.error(f"Error getting prices for {stock_code}: {e}")
                        continue

                # Complete this batch
                if _progress and batch_progress_task:
                    _progress.update(
                        batch_progress_task,
                        completed=batch_idx + 1,
                        description=f"âœ… Batch {batch_idx + 1} completed ({len([r for r in price_results if r['original_idx'] in [idx for idx, _ in batch]])} prices fetched)",
                    )

            # Remove batch progress bar when done
            if _progress and batch_progress_task:
                await asyncio.sleep(0.5)  # Brief pause to show completion
                _progress.remove_task(batch_progress_task)

            # Apply price change filter
            if _progress and _parent_task_id:
                _progress.update(
                    _parent_task_id,
                    completed=70,
                    description="ğŸ” Applying price change filters...",
                )

            # Filter by price change percentage (< max_price_change_percent)
            filtered_stocks = [
                stock
                for stock in price_results
                if stock["price_change_pct"] < self.MAX_PRICE_CHANGE_PERCENT
            ]

            self.logger.info(
                f"After price change filter (<{self.MAX_PRICE_CHANGE_PERCENT}%): {len(filtered_stocks)} stocks"
            )

            if not filtered_stocks:
                self.logger.warning("No stocks passed the price change filter")
                if _progress and _parent_task_id:
                    _progress.update(
                        _parent_task_id,
                        completed=100,
                        description="âš ï¸ No stocks passed the price change filter",
                    )
                return

            # Now enrich the filtered stocks with full data (industry + fund flow)
            if _progress and _parent_task_id:
                _progress.update(
                    _parent_task_id,
                    completed=75,
                    description=f"ğŸ“Š Enriching {len(filtered_stocks)} filtered stocks with fund flow data...",
                )

            # Create a nested progress bar for enrichment tracking
            enrichment_progress_task = None
            if _progress:
                enrichment_progress_task = _progress.add_task(
                    "    ğŸ’° Enriching with fund flow data",
                    total=len(filtered_stocks),
                    visible=True,
                )

            all_results = []

            for i, stock_info in enumerate(filtered_stocks):
                row = stock_info["row"]
                stock_code = str(row["ä»£ç "]).zfill(
                    6
                )  # Convert to string and pad to 6 digits
                stock_name = row["åç§°"]

                # Update main progress
                if _progress and _parent_task_id:
                    progress_pct = 75 + (i / len(filtered_stocks)) * 15
                    _progress.update(
                        _parent_task_id,
                        completed=progress_pct,
                        description=f"ğŸ“Š Enriching stock {i + 1}/{len(filtered_stocks)}: {stock_code}",
                    )

                # Update enrichment progress
                if _progress and enrichment_progress_task:
                    _progress.update(
                        enrichment_progress_task,
                        completed=i,
                        description=f"Processing {stock_code} - {stock_name}",
                    )

                try:
                    # Get fund flow data
                    fund_flow_data = await self.get_fund_flow_data(stock_code)

                    # Get market data
                    market_data = self.get_stock_market_data(stock_code)

                    # Get industry
                    industry = self.get_stock_industry(stock_code)

                    # Build complete result following strict column sequence
                    result = {
                        # Column 1: Row number starting from 0 (blank column name)
                        "": i,
                        # Column 2: è¡Œä¸š
                        "è¡Œä¸š": industry,
                        # Column 3: ä»£ç 
                        "ä»£ç ": stock_code,
                        # Column 4: åç§°
                        "åç§°": stock_name,
                        # Column 5: æ€»å¸‚å€¼(äº¿)
                        "æ€»å¸‚å€¼(äº¿)": market_data.get("æ€»å¸‚å€¼(äº¿)"),
                        # Column 6: æµé€šå¸‚å€¼(äº¿)
                        "æµé€šå¸‚å€¼(äº¿)": market_data.get("æµé€šå¸‚å€¼(äº¿)"),
                        # Column 7: å¸‚ç›ˆç‡-åŠ¨æ€
                        "å¸‚ç›ˆç‡-åŠ¨æ€": market_data.get("å¸‚ç›ˆç‡-åŠ¨æ€"),
                        # Column 8: å¸‚å‡€ç‡
                        "å¸‚å‡€ç‡": market_data.get("å¸‚å‡€ç‡"),
                        # Column 9: é€è½¬è‚¡ä»½-é€è½¬æ€»æ¯”ä¾‹
                        "é€è½¬è‚¡ä»½-é€è½¬æ€»æ¯”ä¾‹": row["é€è½¬è‚¡ä»½-é€è½¬æ€»æ¯”ä¾‹"],
                        # Column 10: é™¤æƒé™¤æ¯æ—¥
                        "é™¤æƒé™¤æ¯æ—¥": stock_info["row"]["é™¤æƒé™¤æ¯æ—¥"].strftime(
                            "%Y-%m-%d"
                        )
                        if isinstance(stock_info["row"]["é™¤æƒé™¤æ¯æ—¥"], datetime)
                        else str(stock_info["row"]["é™¤æƒé™¤æ¯æ—¥"]),
                        # Column 11: é™¤æƒé™¤æ¯æ—¥è‚¡ä»·
                        "é™¤æƒé™¤æ¯æ—¥è‚¡ä»·": stock_info["ex_price"],
                        # Column 12: {today}è‚¡ä»·
                        f"{datetime.now().strftime('%Y%m%d')}è‚¡ä»·": stock_info[
                            "today_price"
                        ],
                        # Column 13: è‡ªé™¤æƒå‡ºæ¯æ—¥èµ·æ¶¨è·Œå¹…(%)
                        "è‡ªé™¤æƒå‡ºæ¯æ—¥èµ·æ¶¨è·Œå¹…(%)": stock_info["price_change_pct"],
                    }

                    # Add dynamic fund flow and price change columns based on configured periods
                    column_index = 14  # Start from column 14
                    for period in self.akshare_config.period_count:
                        # Fund flow columns
                        fund_flow_key = f"{period}æ—¥ä¸»åŠ›å‡€æµå…¥-æ€»å‡€é¢(äº¿)"
                        result[fund_flow_key] = fund_flow_data.get(fund_flow_key)
                        column_index += 1

                    for period in self.akshare_config.period_count:
                        # Price change columns
                        price_change_key = f"{period}æ—¥æ¶¨è·Œå¹…(%)"
                        result[price_change_key] = fund_flow_data.get(price_change_key)
                        column_index += 1

                    # Add final market data columns
                    result.update(
                        {
                            # 60æ—¥æ¶¨è·Œå¹…(%)
                            "60æ—¥æ¶¨è·Œå¹…(%)": market_data.get("60æ—¥æ¶¨è·Œå¹…(%)"),
                            # å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…(%)
                            "å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…(%)": market_data.get("å¹´åˆè‡³ä»Šæ¶¨è·Œå¹…(%)"),
                        }
                    )

                    all_results.append(result)

                    # Update enrichment progress with success
                    if _progress and enrichment_progress_task:
                        _progress.update(
                            enrichment_progress_task,
                            completed=i + 1,
                            description=f"âœ… {stock_code} - {stock_name} completed",
                        )

                except Exception as e:
                    self.logger.error(f"Error enriching stock {stock_code}: {e}")
                    # Update enrichment progress with error
                    if _progress and enrichment_progress_task:
                        _progress.update(
                            enrichment_progress_task,
                            completed=i + 1,
                            description=f"âŒ {stock_code} - {stock_name} failed",
                        )
                    continue

            # Remove enrichment progress bar when done
            if _progress and enrichment_progress_task:
                await asyncio.sleep(0.5)  # Brief pause to show completion
                _progress.remove_task(enrichment_progress_task)

            if _progress and _parent_task_id:
                _progress.update(
                    _parent_task_id, completed=90, description="ğŸ“ Generating report..."
                )

            # Create DataFrame and save report
            if all_results:
                result_df = pd.DataFrame(all_results)

                # Sort by price change percentage
                result_df = result_df.sort_values(
                    by=["è‡ªé™¤æƒå‡ºæ¯æ—¥èµ·æ¶¨è·Œå¹…(%)"], ascending=True
                )

                # Reset the first column to sequential 0-based indexing after sorting
                result_df.iloc[:, 0] = pd.Series(
                    range(len(result_df)), index=result_df.index
                )

                # Create output directory
                os.makedirs(self.REPORT_DIR, exist_ok=True)

                # Generate output filename
                today_str = datetime.now().strftime("%Y%m%d")
                output_filename = self.OUTPUT_FILENAME_TEMPLATE.format(date=today_str)
                output_path = os.path.join(self.REPORT_DIR, output_filename)

                # Save to CSV
                result_df.to_csv(output_path, index=False, encoding="utf-8-sig")

                self.logger.info(
                    f"FHPS analysis completed. Report saved to: {output_path}"
                )
                self.logger.info(f"Total stocks processed: {len(result_df)}")
            else:
                self.logger.warning("No results to save - all stock processing failed")

            if _progress and _parent_task_id:
                _progress.update(
                    _parent_task_id,
                    completed=100,
                    description="âœ… FHPS analysis completed",
                )
                self.logger.info("Progress updated to 100% - Analysis completed")

        except Exception as e:
            error_msg = f"FHPS filter analysis failed: {str(e)}"
            self.logger.error(error_msg)
            if _progress and _parent_task_id:
                _progress.update(_parent_task_id, description=f"âŒ {error_msg}")
            raise
